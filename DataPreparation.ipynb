{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycoshark in /opt/conda/lib/python3.7/site-packages (1.2.7)\n",
      "Requirement already satisfied: mongoengine in /opt/conda/lib/python3.7/site-packages (from pycoshark) (0.19.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from pycoshark) (2.8.1)\n",
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.7/site-packages (from pycoshark) (3.10.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from pycoshark) (2.4)\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.7/site-packages (from pycoshark) (0.12.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from mongoengine->pycoshark) (1.13.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->pycoshark) (4.4.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from python-Levenshtein->pycoshark) (42.0.2.post20191201)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.13.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think you don't have to execute this when running on your own device\n",
    "\n",
    "# code for installing our own library for accessing the MongoDB through a ORM engine\n",
    "import sys\n",
    "!{sys.executable} -m pip install pycoshark\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import string\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import pickle\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "from mongoengine import connect\n",
    "from pycoshark.mongomodels import People, Commit, Project, VCSSystem, Issue, IssueSystem\n",
    "from pycoshark.utils import create_mongodb_uri_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['134.76.81.151:27017'], document_class=dict, tz_aware=False, connect=True, authsource='smartshark', read_preference=Primary())"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Database credentials\n",
    "user = 'datascience2019'\n",
    "password = 'zE3qHdeJtdVJYznf'\n",
    "host = '134.76.81.151'\n",
    "port = '27017'\n",
    "authentication_db = 'smartshark'\n",
    "database = \"smartshark\"\n",
    "ssl_enabled = None\n",
    "\n",
    "# Establish connection\n",
    "uri = create_mongodb_uri_string(user, password, host, port, authentication_db, ssl_enabled)\n",
    "connect(database, host=uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data and generate data structures to store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_issue(issue):\n",
    "    print(issue.issue_type)\n",
    "    print(issue.title)\n",
    "    print(issue.desc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text, name = None):\n",
    "    # remove links\n",
    "    text = re.sub('www\\.\\\\S*', '', text)\n",
    "    text = re.sub('https:\\\\S*', '', text)\n",
    "    text = re.sub('https:\\\\S*', '', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    # Tokenize each document into word list\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_word = tokenizer.tokenize(text)\n",
    "    \n",
    "    # remove other common words like project names\n",
    "    # Unfortunately, this does not seem to improve performance at all, but do experiment!\n",
    "#     custom_stop_words = ['org', 'apache']\n",
    "#     if name != None:\n",
    "#         custom_stop_words += name.split('-')\n",
    "#     for word in custom_stop_words:\n",
    "#         tokenized_word = [re.sub(word + '[\\\\s\\\\.$]', \"\", word) for word in tokenized_word]\n",
    "#         text = re.sub(word + '[\\\\s\\\\.$]', '', text)\n",
    "    \n",
    "    # Remove any digits or underscores in each word of the list\n",
    "    tokenized_word_digits_removed=[re.sub(r\"\\d+|_+\", \"\", word) for word in tokenized_word]\n",
    "    filtered_words=[]\n",
    "    for word in tokenized_word_digits_removed:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_words = [snowball_stemmer.stem(word) for word in lemmatized_words]\n",
    "    return ' '.join(stemmed_words)\n",
    "    \n",
    "    \n",
    "def Nonetostr(string):\n",
    "    if string is None:\n",
    "        string = \"\"\n",
    "    return string\n",
    "\n",
    "\n",
    "def make_matrix(corpus, n_words = 1000):\n",
    "    vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', max_features = n_words)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer.get_feature_names(), X\n",
    "\n",
    "def make_count_matrix(corpus, n_words = 10000):\n",
    "    vectorizer = CountVectorizer(lowercase=True, stop_words='english', max_features = n_words)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer.get_feature_names(), X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text_2(text, name = None):\n",
    "    attributes = []\n",
    "    \n",
    "    text_length = len(text)\n",
    "    \n",
    "    # remove links\n",
    "    text = re.sub('www\\.\\\\S*', '', text)\n",
    "    text = re.sub('http:\\\\S*', '', text)\n",
    "    text = re.sub('https:\\\\S*', '', text)\n",
    "    \n",
    "    if len(text) < text_length:\n",
    "        attributes.append('contains_link')\n",
    "        text_length = len(text)\n",
    "    \n",
    "    if re.search('[Ee]rror', text) != None or re.search('[Ee]xception', text) != None:\n",
    "        attributes.append('contains_exception')\n",
    "    \n",
    "    # removes empty line\n",
    "    re.sub(r'\\n(\\s)*\\n', '\\n', text)\n",
    "    text_length = len(text)\n",
    "    \n",
    "    # removes indented lines\n",
    "    text = re.sub('(^|\\n)(  +|\\t).*', '\\n', text)\n",
    "    # removes java style multiline comments: /* ... */\n",
    "    #text = re.sub(r'\\/\\*\\*?([^*]+(\\*)?[^/*]+)+\\*\\/', '', text)\n",
    "    text = re.sub(r'\\/(\\*)(.|\\n)*?\\*\\/', '', text)\n",
    "    # removes camel-case strings\n",
    "    text = re.sub(r'[a-z]*([A-Z]+[a-z]+)+', '', text)\n",
    "    if len(text) < text_length:\n",
    "        attributes.append('contains_code')\n",
    "        text_length = len(text)\n",
    "        \n",
    "#     text = ''.join(filter(lambda x: x in string.printable, text))\n",
    "#     if len(text) < text_length:\n",
    "#         attributes.append('contains_non_english_char')\n",
    "    \n",
    "    \n",
    "    \n",
    "    text = text.lower()\n",
    "    # Tokenize each document into word list\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_word = tokenizer.tokenize(text)\n",
    "\n",
    "    # Remove any digits or underscores in each word of the list\n",
    "    tokenized_word=[re.sub(r\"\\d+|_+\", \"\", word) for word in tokenized_word]\n",
    "    \n",
    "    # remove other common words like project names\n",
    "    # Unfortunately, this does not seem to improve performance at all, but do experiment!\n",
    "    custom_stop_words = ['apache', 'aaa+', 'abc', 'bb+', 'cc+', 'zz+', 'xx+', 'yy+'] # 'org'\n",
    "    if name != None:\n",
    "        custom_stop_words += name.split('-')\n",
    "    for stop_word in custom_stop_words:\n",
    "        tokenized_word = [word if re.match(stop_word, word) == None else '' for word in tokenized_word]\n",
    "        #tokenized_word = [re.sub(stop_word + '(\\s|\\.|$)', '', word) for word in tokenized_word]\n",
    "        #text = re.sub(word + '[\\\\s\\\\.$]', '', text)   \n",
    "        \n",
    "    contains_non_english_char = False\n",
    "    filtered_words=[]\n",
    "    for word in tokenized_word:\n",
    "        if None != re.match('[a-z][a-z]+$', word):\n",
    "            if word not in stop_words:\n",
    "                filtered_words.append(word)\n",
    "        else:\n",
    "            contains_non_english_char = True\n",
    "    if contains_non_english_char:\n",
    "        attributes.append('contains_non_english_char')\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_words = [snowball_stemmer.stem(word) for word in lemmatized_words]\n",
    "    \n",
    "    return ' '.join(stemmed_words), attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox cell to inspect issues or test stuff\n",
    "\n",
    "# project = Project.objects(name='commons-math').only('id').get()\n",
    "# issue_system = IssueSystem.objects(project_id=project.id).only('id').get()\n",
    "# issues = Issue.objects(issue_system_id=issue_system.id).only(*['issue_type','title','desc','priority','created_at'])\n",
    "\n",
    "#preprocess_text_2('aaa abc hello sub zzz rüm', 'ant-ivy')\n",
    "#re.match('[a-z][a-z]+$', 'aoã')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create stemmed corpus without stopwords and links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raw_data():\n",
    "    names = ['ant-ivy','archiva','calcite','cayenne','commons-bcel','commons-beanutils','commons-codec','commons-collections','commons-compress','commons-configuration','commons-dbcp','commons-digester','commons-io','commons-jcs','commons-jexl','commons-lang','commons-math','commons-net','commons-rdf','commons-scxml','commons-validator','commons-vfs','deltaspike','eagle','giraph','gora','jspwiki','kylin','lens','mahout','manifoldcf','nutch','opennlp','parquet-mr','santuario-java','systemml','tika','wss4j']\n",
    "    # Prepare csv file to store the information in\n",
    "    csv_columns = ['issue_type','title_and_desc','priority', 'environment', 'original_time_estimate', 'created_at']\n",
    "    csv_file = \"issues.csv\"\n",
    "    f = open(csv_file, 'w+', newline='', encoding=\"utf8\")\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # corpus will consist of title and desc\n",
    "    corpus = []\n",
    "\n",
    "    # Keep tally of type frequencies\n",
    "    all_types = ['None']\n",
    "    all_count = dict()\n",
    "    all_count['None'] = 0\n",
    "\n",
    "    index = 0\n",
    "    for name in names:\n",
    "        project = Project.objects(name=name).only('id').get()\n",
    "        issue_system = IssueSystem.objects(project_id=project.id).only('id').get()\n",
    "\n",
    "        issue_features = ['original_time_estimate', 'created_at', 'priority', 'creator_id', 'title', 'desc', 'issue_type']\n",
    "        # removed environment from extracted features\n",
    "\n",
    "        for issue in Issue.objects(issue_system_id=issue_system.id).only(*issue_features):\n",
    "            index += 1\n",
    "            issue_dict = dict()\n",
    "            t = issue.issue_type\n",
    "            if t == None:\n",
    "                all_count['None'] += 1\n",
    "                #print_issue(issue)\n",
    "                continue # all the relevant fields are None as well, so skip this issue\n",
    "            if not issue.issue_type in all_types:\n",
    "                all_types.append(issue.issue_type)\n",
    "                all_count[issue.issue_type] = 1\n",
    "            all_count[issue.issue_type] += 1\n",
    "            issue_dict[\"issue_type\"] = t\n",
    "            if issue.title == None and issue.desc == None:\n",
    "                print_issue(issue)\n",
    "                continue\n",
    "\n",
    "            issue_dict[\"title_and_desc\"] = preprocess_text(Nonetostr(issue.title) + \" \" + Nonetostr(issue.desc))\n",
    "            issue_dict[\"priority\"] = issue.priority\n",
    "            issue_dict[\"original_time_estimate\"] = issue.original_time_estimate\n",
    "            issue_dict[\"created_at\"] = issue.created_at\n",
    "\n",
    "            text = preprocess_text(Nonetostr(issue.title) + \" \" + Nonetostr(issue.desc), name)\n",
    "            corpus.append(text)\n",
    "\n",
    "\n",
    "            try:\n",
    "                writer.writerow(issue_dict)\n",
    "            except IOError:\n",
    "                print(\"I/O error\")\n",
    "\n",
    "    f.close()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create corpus without code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raw_data_2():\n",
    "    names = ['ant-ivy','archiva','calcite','cayenne','commons-bcel','commons-beanutils','commons-codec','commons-collections','commons-compress','commons-configuration','commons-dbcp','commons-digester','commons-io','commons-jcs','commons-jexl','commons-lang','commons-math','commons-net','commons-rdf','commons-scxml','commons-validator','commons-vfs','deltaspike','eagle','giraph','gora','jspwiki','kylin','lens','mahout','manifoldcf','nutch','opennlp','parquet-mr','santuario-java','systemml','tika','wss4j']\n",
    "    # Prepare csv file to store the information in\n",
    "    csv_columns = ['issue_type','title_and_desc','priority','created_at','code','exception','link','non_english_char']\n",
    "    csv_file = \"issues_2.csv\"\n",
    "    f = open(csv_file, 'w+', newline='', encoding=\"utf8\")\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # corpus will consist of title and desc\n",
    "    corpus = []\n",
    "\n",
    "    index = 0\n",
    "    for name in names:\n",
    "        project = Project.objects(name=name).only('id').get()\n",
    "        issue_system = IssueSystem.objects(project_id=project.id).only('id').get()\n",
    "\n",
    "        issue_features = ['original_time_estimate', 'created_at', 'priority', 'creator_id', 'title', 'desc', 'issue_type']\n",
    "        # removed environment from extracted features\n",
    "\n",
    "        for issue in Issue.objects(issue_system_id=issue_system.id).only(*issue_features):\n",
    "            index += 1\n",
    "            issue_dict = dict()\n",
    "            t = issue.issue_type\n",
    "            if t == None:\n",
    "                if issue.title != None or issue.desc != None:\n",
    "                    print_issue(issue)\n",
    "                continue # all the relevant fields are None as well, so skip this issue\n",
    "            issue_dict[\"issue_type\"] = t\n",
    "            if issue.title == None and issue.desc == None:\n",
    "                continue\n",
    "\n",
    "            issue_dict[\"title_and_desc\"] = preprocess_text_2(Nonetostr(issue.title) + \" \" + Nonetostr(issue.desc))\n",
    "            issue_dict[\"priority\"] = issue.priority\n",
    "            #issue_dict[\"original_time_estimate\"] = issue.original_time_estimate\n",
    "            issue_dict[\"created_at\"] = issue.created_at\n",
    "\n",
    "            text, flags = preprocess_text_2(Nonetostr(issue.title) + \" \" + Nonetostr(issue.desc), name)\n",
    "            corpus.append(text)\n",
    "            issue_dict['code'] = 'contains_code' in flags\n",
    "            issue_dict['exception'] = 'contains_exception' in flags\n",
    "            issue_dict['link'] = 'contains_link' in flags\n",
    "            issue_dict['non_english_char'] = 'contains_non_english_char' in flags\n",
    "\n",
    "            try:\n",
    "                writer.writerow(issue_dict)\n",
    "            except IOError:\n",
    "                print(\"I/O error\")\n",
    "\n",
    "    f.close()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this cell with the generate-method and file names of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = generate_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [s if type(s) == str else '' for s in corpus]\n",
    "words, matrix = make_matrix(corpus, 1000)\n",
    "save_npz('matrix_2', matrix)\n",
    "with open(\"feature_names_2.txt\", \"wb\") as fp:\n",
    "        pickle.dump(words, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abil', 'abl', 'abstract', 'accept', 'access', 'accord', 'account', 'action', 'activ', 'actual', 'ad', 'adapt', 'adb', 'add', 'addit', 'address', 'admin', 'affect', 'agent', 'aggreg', 'aim', 'alert', 'algorithm', 'allow', 'alpha', 'alreadi', 'altern', 'alway', 'annot', 'anoth', 'ant', 'anyth', 'anywher', 'api', 'apidoc', 'app', 'appear', 'append', 'appli', 'applic', 'approach', 'appropri', 'archiv', 'arg', 'argument', 'arquillian', 'array', 'artifact', 'ask', 'assembl', 'assign', 'assum', 'attach', 'attempt', 'attribut', 'authent', 'author', 'auto', 'automat', 'auxiliari', 'avail', 'avatica', 'avoid', 'avro', 'awt', 'baafb', 'bad', 'bar', 'base', 'basic', 'batch', 'bean', 'beanutil', 'becom', 'begin', 'behavior', 'behaviour', 'believ', 'belong', 'best', 'beta', 'better', 'big', 'bin', 'binari', 'bind', 'bio', 'bit', 'block', 'bodi', 'boolean', 'box', 'branch', 'break', 'broken', 'buffer', 'bug', 'build', 'builder', 'built', 'bundl', 'button', 'byte', 'ca', 'cach', 'calcul', 'case', 'cast', 'catalina', 'catch', 'categor', 'caus', 'cdh', 'central', 'certain', 'cetrea', 'cf', 'chain', 'chang', 'channel', 'char', 'charact', 'charset', 'check', 'checksum', 'child', 'chum', 'ci', 'class', 'classifi', 'classpath', 'claus', 'cldr', 'clean', 'cleanup', 'clear', 'cli', 'click', 'client', 'clone', 'close', 'cluster', 'code', 'codec', 'codegen', 'codehaus', 'col', 'collect', 'color', 'column', 'com', 'combin', 'come', 'comm', 'command', 'comment', 'commit', 'common', 'compar', 'comparison', 'compat', 'compil', 'complet', 'complex', 'compon', 'compress', 'comput', 'concurr', 'condit', 'conf', 'config', 'configur', 'configurablepictur', 'conflict', 'confus', 'connect', 'connector', 'consid', 'consist', 'constant', 'constraint', 'construct', 'constructor', 'consum', 'contain', 'content', 'context', 'continu', 'contrib', 'contribut', 'control', 'convers', 'convert', 'copi', 'core', 'correct', 'correspond', 'cost', 'count', 'cours', 'cp', 'cpp', 'crawl', 'crawler', 'creat', 'creation', 'cst', 'csv', 'cube', 'cubev', 'cuboid', 'current', 'custom', 'daemon', 'data', 'databas', 'dataset', 'date', 'day', 'db', 'dc', 'deal', 'debug', 'declar', 'decod', 'default', 'defin', 'definit', 'delet', 'depend', 'deploy', 'deprec', 'deptno', 'deriv', 'describ', 'descript', 'descriptor', 'design', 'detect', 'determin', 'dev', 'develop', 'df', 'dictionari', 'diff', 'differ', 'digest', 'dimens', 'dir', 'direct', 'directori', 'disabl', 'discuss', 'disk', 'display', 'distanc', 'distinct', 'distribut', 'dk', 'dml', 'doc', 'document', 'dom', 'domain', 'doubl', 'download', 'driver', 'drop', 'dt', 'dump', 'duplic', 'dynam', 'easi', 'easier', 'easili', 'eclips', 'edg', 'edit', 'editablecontrol', 'effect', 'effici', 'ejb', 'ekoontz', 'elaps', 'element', 'els', 'email', 'embed', 'emp', 'empti', 'enabl', 'encod', 'encount', 'encrypt', 'end', 'engin', 'enhanc', 'ensur', 'enter', 'entir', 'entiti', 'entri', 'enum', 'env', 'environ', 'equal', 'error', 'escap', 'especi', 'estim', 'evalu', 'event', 'everi', 'everyth', 'exact', 'exampl', 'exclud', 'exec', 'execut', 'exist', 'expect', 'explain', 'explicit', 'expos', 'express', 'extend', 'extens', 'extern', 'extra', 'extract', 'fa', 'fact', 'factori', 'fail', 'failur', 'fals', 'famili', 'fapi', 'far', 'featur', 'fetch', 'field', 'file', 'filenam', 'filter', 'final', 'fine', 'finish', 'fit', 'fix', 'flag', 'float', 'folder', 'follow', 'foo', 'forc', 'form', 'format', 'frame', 'framework', 'ftp', 'function', 'futur', 'ga', 'gc', 'general', 'generat', 'generic', 'git', 'given', 'glassfish', 'global', 'goal', 'good', 'googl', 'got', 'graph', 'great', 'grizzli', 'group', 'guess', 'hadoop', 'handl', 'handler', 'hang', 'happen', 'hard', 'hash', 'hbase', 'hdfs', 'head', 'header', 'help', 'high', 'hit', 'hive', 'home', 'host', 'hour', 'howev', 'href', 'hsqldb', 'html', 'http', 'httpclient', 'ibm', 'id', 'idea', 'identifi', 'idf', 'ignor', 'imag', 'impl', 'implement', 'import', 'improv', 'includ', 'incorrect', 'increas', 'incub', 'index', 'indic', 'inf', 'info', 'inform', 'inherit', 'init', 'initi', 'inject', 'inner', 'input', 'insert', 'insid', 'instal', 'instanc', 'instanti', 'instead', 'instruct', 'int', 'integ', 'integr', 'interceptor', 'interfac', 'intern', 'interop', 'interv', 'introduc', 'invalid', 'invok', 'io', 'issu', 'item', 'iter', 'jackson', 'jamba', 'jar', 'java', 'javadoc', 'javax', 'jboss', 'jdbc', 'jdk', 'jenkin', 'jersey', 'jetti', 'jira', 'job', 'join', 'jonathan', 'json', 'jsp', 'junit', 'jvm', 'kb', 'key', 'kind', 'know', 'known', 'kris', 'label', 'lang', 'languag', 'larg', 'later', 'latest', 'layer', 'ldap', 'le', 'lead', 'leak', 'left', 'length', 'let', 'level', 'lib', 'librari', 'licens', 'like', 'limit', 'line', 'link', 'list', 'listen', 'liter', 'littl', 'load', 'loader', 'local', 'localhost', 'locat', 'lock', 'log', 'logic', 'login', 'logj', 'long', 'longer', 'look', 'lookup', 'loop', 'lot', 'lp', 'lucen', 'machin', 'mahout', 'mail', 'main', 'maintain', 'make', 'manag', 'mani', 'manual', 'map', 'mapper', 'mapr', 'mapreduc', 'mark', 'master', 'match', 'math', 'matrix', 'maven', 'max', 'mayb', 'mb', 'mcf', 'md', 'mean', 'measur', 'mechan', 'memori', 'mention', 'merg', 'messag', 'meta', 'metadata', 'metastor', 'method', 'metric', 'migrat', 'mime', 'min', 'minut', 'miss', 'ml', 'mode', 'model', 'modifi', 'modul', 'monitor', 'mortbay', 'mr', 'multi', 'multipl', 'mung', 'mvn', 'mysql', 'na', 'namespac', 'nativ', 'necessari', 'need', 'nest', 'net', 'netti', 'network', 'new', 'nice', 'nid', 'node', 'noformat', 'non', 'normal', 'note', 'noth', 'notic', 'npe', 'null', 'number', 'numer', 'object', 'objectstyl', 'observ', 'occur', 'offset', 'ok', 'old', 'open', 'opensymphoni', 'oper', 'opt', 'optim', 'option', 'order', 'org', 'origin', 'output', 'overrid', 'overwrit', 'packag', 'page', 'parallel', 'param', 'paramet', 'parent', 'pars', 'parser', 'partial', 'particular', 'partit', 'pas', 'pass', 'password', 'patch', 'path', 'pattern', 'pdf', 'peopl', 'perform', 'permiss', 'persist', 'pick', 'pk', 'place', 'plain', 'plan', 'platform', 'pleas', 'plexus', 'plugin', 'pm', 'poi', 'point', 'polici', 'pom', 'pool', 'port', 'posit', 'possibl', 'post', 'potenti', 'pre', 'predic', 'prefer', 'prefix', 'prepar', 'present', 'prevent', 'previous', 'primit', 'print', 'println', 'prio', 'privat', 'probabl', 'problem', 'process', 'produc', 'product', 'profil', 'program', 'project', 'prop', 'proper', 'properti', 'propos', 'protect', 'protocol', 'provid', 'proxi', 'public', 'publish', 'pull', 'purpos', 'push', 'qualifi', 'queri', 'question', 'queue', 'quick', 'quit', 'quot', 'random', 'rang', 'rc', 'rdd', 'read', 'readi', 'real', 'realli', 'reason', 'receiv', 'recent', 'recogn', 'recommend', 'record', 'redback', 'redirect', 'reduc', 'refactor', 'refer', 'reflect', 'refresh', 'regard', 'regex', 'region', 'regist', 'regress', 'regular', 'rel', 'relat', 'relationship', 'releas', 'reli', 'remain', 'remot', 'remov', 'renam', 'replac', 'repo', 'report', 'repositori', 'reproduc', 'request', 'requir', 'reset', 'resolut', 'resolv', 'resourc', 'respect', 'respons', 'rest', 'restart', 'result', 'retriev', 'return', 'reus', 'revers', 'review', 'revis', 'rewrit', 'right', 'rmvar', 'root', 'row', 'rule', 'run', 'runtim', 'sa', 'safe', 'saml', 'sampl', 'save', 'sax', 'say', 'scala', 'scalar', 'scale', 'scan', 'scenario', 'schedul', 'schema', 'scope', 'score', 'script', 'search', 'sec', 'second', 'section', 'secur', 'seen', 'segment', 'select', 'send', 'sens', 'sent', 'separ', 'sequenc', 'serial', 'server', 'servic', 'servlet', 'session', 'set', 'setup', 'sever', 'sh', 'sha', 'share', 'short', 'shown', 'sign', 'signatur', 'similar', 'simpl', 'simpli', 'simplifi', 'sinc', 'singl', 'site', 'situat', 'size', 'skip', 'slave', 'slfj', 'slow', 'small', 'snapshot', 'socket', 'solr', 'solut', 'solv', 'someth', 'sometim', 'sort', 'sourc', 'space', 'spark', 'spars', 'spec', 'special', 'specif', 'specifi', 'spi', 'split', 'spring', 'springframework', 'sql', 'src', 'stabl', 'stack', 'stage', 'standalon', 'standard', 'start', 'stat', 'state', 'statement', 'static', 'statist', 'status', 'step', 'stop', 'storag', 'store', 'str', 'strategi', 'stream', 'string', 'structur', 'style', 'sub', 'subclass', 'submit', 'success', 'suggest', 'sum', 'sun', 'support', 'sure', 'surefir', 'svn', 'swing', 'switch', 'symbol', 'synchron', 'syntax', 'sysml', 'tab', 'tabl', 'tag', 'tar', 'target', 'task', 'tast', 'td', 'tell', 'temp', 'templat', 'term', 'test', 'testcub', 'text', 'therefor', 'thing', 'think', 'thread', 'thrift', 'throw', 'thrown', 'tid', 'tika', 'time', 'timeout', 'timer', 'timestamp', 'titl', 'tmp', 'token', 'tomcat', 'tool', 'topolog', 'total', 'trace', 'track', 'train', 'transact', 'transfer', 'transform', 'transient', 'transit', 'translat', 'treat', 'tree', 'tri', 'trigger', 'true', 'trunk', 'turn', 'txt', 'type', 'ui', 'understand', 'uniqu', 'unit', 'unknown', 'unnecessari', 'updat', 'upgrad', 'upload', 'uri', 'url', 'usag', 'use', 'user', 'usernam', 'usr', 'utf', 'util', 'valid', 'valu', 'variabl', 'various', 'vector', 'verifi', 'version', 'vertex', 'view', 'vm', 'void', 'wait', 'want', 'war', 'warn', 'way', 'web', 'webapp', 'websit', 'weld', 'wiki', 'window', 'word', 'work', 'workaround', 'worker', 'workspac', 'wrap', 'write', 'written', 'wrong', 'wssj', 'xerc', 'xml', 'xmlns', 'xwork', 'yarn', 'year', 'zero', 'zip', 'zookeep']\n"
     ]
    }
   ],
   "source": [
    "# inspect the results\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this cell in order to load data for further manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"issues.csv\")\n",
    "mat = load_npz('matrix.npz')\n",
    "type_list = df.issue_type.to_list()\n",
    "corpus = df.title_and_desc.to_list()\n",
    "corpus = [s if type(s) == str else '' for s in corpus] # there is one issue without description in the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some statistics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Blogs - New Blog User Account Request': 1, 'Improvement': 12535, 'Planned Work': 1, 'Umbrella': 8, 'New Feature': 3283, 'Brainstorming': 2, 'Blog - New Blog Request': 3, 'Technical task': 5, 'Documentation': 71, 'Dependency upgrade': 9, 'New JIRA Project': 2, 'Temp': 1, 'Project': 1, 'Proposal': 1, 'Bug': 21129, 'Story': 12, 'New Git Repo': 1, 'Epic': 91, 'Sub-task': 1870, 'Wish': 550, 'Question': 85, 'Test': 281, 'Task': 2905, 'New TLP ': 1}\n",
      "Relative frequencies: {'Blogs - New Blog User Account Request': 2.333831217326363e-05, 'Improvement': 0.2925457430918596, 'Planned Work': 2.333831217326363e-05, 'Umbrella': 0.00018670649738610905, 'New Feature': 0.0766196788648245, 'Brainstorming': 4.667662434652726e-05, 'Blog - New Blog Request': 7.001493651979089e-05, 'Technical task': 0.00011669156086631815, 'Documentation': 0.0016570201643017178, 'Dependency upgrade': 0.00021004480955937266, 'New JIRA Project': 4.667662434652726e-05, 'Temp': 2.333831217326363e-05, 'Project': 2.333831217326363e-05, 'Proposal': 2.333831217326363e-05, 'Bug': 0.49311519790888725, 'Story': 0.00028005974607916355, 'New Git Repo': 2.333831217326363e-05, 'Epic': 0.00212378640776699, 'Sub-task': 0.04364264376400299, 'Wish': 0.012836071695294996, 'Question': 0.0019837565347274083, 'Test': 0.0065580657206870795, 'Task': 0.06779779686333084, 'New TLP ': 2.333831217326363e-05}\n",
      "['Improvement', 'New Feature', 'Bug', 'Sub-task', 'Task']\n",
      "['Improvement', 'New Feature', 'Bug', 'Sub-task', 'Wish', 'Task']\n"
     ]
    }
   ],
   "source": [
    "n_issues = len(corpus)\n",
    "all_types = set(type_list)\n",
    "\n",
    "all_count = dict()\n",
    "for t in all_types:\n",
    "    all_count[t] = type_list.count(t)\n",
    "    \n",
    "print(all_count)\n",
    "\n",
    "\n",
    "rel_freq = all_count.copy()\n",
    "for key in all_types:\n",
    "    rel_freq[key] = all_count[key]/n_issues\n",
    "print(\"Relative frequencies: \" + str(rel_freq))\n",
    "\n",
    "\n",
    "relevant_classes1000 = [t for t in all_types if all_count[t] >= 1000 ]\n",
    "#[y if y == 'Bug' else 'Other' for y in y_train]\n",
    "print(relevant_classes1000)\n",
    "relevant_classes400 = [t for t in all_types if all_count[t] >= 400 ]\n",
    "#[y if y == 'Bug' else 'Other' for y in y_train]\n",
    "print(relevant_classes400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new data structures in which raw classes are eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bug', 'Improvement', 'New Feature', 'Sub-task', 'Task'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rid(class_arr, name_extension, df = df, n_words = 1000):\n",
    "    df2 = df.copy()\n",
    "    df2 = df2[df2.issue_type.isin(class_arr)]\n",
    "    \n",
    "    corpus2 = df2.title_and_desc.to_list()\n",
    "    corpus2 = [s if type(s) == str else '' for s in corpus2]\n",
    "    mat_corpus2 = make_matrix(corpus2, n_words)\n",
    "    \n",
    "    save_npz('matrix_reduced' + name_extension, mat_corpus2[1])\n",
    "    with open('feature_names_reduced'+name_extension+'.txt', \"wb\") as fp:\n",
    "        pickle.dump(mat_corpus2[0], fp)\n",
    "    df2.to_csv('issues_reduced' + name_extension + '.csv')\n",
    "    \n",
    "    return df2\n",
    "\n",
    "df1000 = get_rid(relevant_classes1000, '')\n",
    "set(df1000.issue_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df400 = get_rid(relevant_classes400, '_plus_wish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "\n",
    "Apply various method to add new data with rare classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_type</th>\n",
       "      <th>title_and_desc</th>\n",
       "      <th>priority</th>\n",
       "      <th>created_at</th>\n",
       "      <th>code</th>\n",
       "      <th>exception</th>\n",
       "      <th>link</th>\n",
       "      <th>non_english_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38928</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>('releas relat improv', ['contains_code', 'con...</td>\n",
       "      <td>Major</td>\n",
       "      <td>2017-01-31 05:09:46.563000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37488</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>('conveni umbrella track improv trigger work d...</td>\n",
       "      <td>Major</td>\n",
       "      <td>2016-03-22 18:38:17.828000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39430</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>('ml jira place understand ml machin learn pla...</td>\n",
       "      <td>Major</td>\n",
       "      <td>2017-07-19 09:24:31.756000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39430</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>('ml jira place understand ml machin learn pla...</td>\n",
       "      <td>Major</td>\n",
       "      <td>2017-07-19 09:24:31.756000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38802</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>('releas relat improv', ['contains_code', 'con...</td>\n",
       "      <td>Major</td>\n",
       "      <td>2016-10-05 02:00:12.390000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39442</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>('creat issu type umbrella', ['contains_code',...</td>\n",
       "      <td>Trivial</td>\n",
       "      <td>2017-07-26 00:07:49.116000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39331</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>('exampl includ notebook exampl document inclu...</td>\n",
       "      <td>Major</td>\n",
       "      <td>2017-05-12 00:23:48.910000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39442</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>('creat issu type umbrella', ['contains_code',...</td>\n",
       "      <td>Trivial</td>\n",
       "      <td>2017-07-26 00:07:49.116000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37488</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>('conveni umbrella track improv trigger work d...</td>\n",
       "      <td>Major</td>\n",
       "      <td>2016-03-22 18:38:17.828000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38928</th>\n",
       "      <td>Umbrella</td>\n",
       "      <td>('releas relat improv', ['contains_code', 'con...</td>\n",
       "      <td>Major</td>\n",
       "      <td>2017-01-31 05:09:46.563000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      issue_type                                     title_and_desc priority  \\\n",
       "38928   Umbrella  ('releas relat improv', ['contains_code', 'con...    Major   \n",
       "37488   Umbrella  ('conveni umbrella track improv trigger work d...    Major   \n",
       "39430   Umbrella  ('ml jira place understand ml machin learn pla...    Major   \n",
       "39430   Umbrella  ('ml jira place understand ml machin learn pla...    Major   \n",
       "38802   Umbrella  ('releas relat improv', ['contains_code', 'con...    Major   \n",
       "...          ...                                                ...      ...   \n",
       "39442   Umbrella  ('creat issu type umbrella', ['contains_code',...  Trivial   \n",
       "39331   Umbrella  ('exampl includ notebook exampl document inclu...    Major   \n",
       "39442   Umbrella  ('creat issu type umbrella', ['contains_code',...  Trivial   \n",
       "37488   Umbrella  ('conveni umbrella track improv trigger work d...    Major   \n",
       "38928   Umbrella  ('releas relat improv', ['contains_code', 'con...    Major   \n",
       "\n",
       "                       created_at  code  exception   link  non_english_char  \n",
       "38928  2017-01-31 05:09:46.563000  True      False  False              True  \n",
       "37488  2016-03-22 18:38:17.828000  True      False  False              True  \n",
       "39430  2017-07-19 09:24:31.756000  True      False  False              True  \n",
       "39430  2017-07-19 09:24:31.756000  True      False  False              True  \n",
       "38802  2016-10-05 02:00:12.390000  True      False  False              True  \n",
       "...                           ...   ...        ...    ...               ...  \n",
       "39442  2017-07-26 00:07:49.116000  True      False  False              True  \n",
       "39331  2017-05-12 00:23:48.910000  True      False  False             False  \n",
       "39442  2017-07-26 00:07:49.116000  True      False  False              True  \n",
       "37488  2016-03-22 18:38:17.828000  True      False  False              True  \n",
       "38928  2017-01-31 05:09:46.563000  True      False  False              True  \n",
       "\n",
       "[400 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_umbrella = df.loc[df['issue_type'] == 'Umbrella']\n",
    "df_minority_upsampled = resample(df_umbrella, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=400,    # to match majority class\n",
    "                                 random_state=1505) # reproducible results\n",
    "df_minority_upsampled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
